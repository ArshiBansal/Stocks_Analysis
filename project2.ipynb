{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7353254f",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbe784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 1. CORE & DATA HANDLING ======================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 2. TIME SERIES CLASSICAL ======================\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 3. SUPERVISED ML (scikit-learn) ======================\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report,mean_absolute_percentage_error,explained_variance_score,max_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69970aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 4. UNSUPERVISED ML ======================\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b53e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder (TensorFlow example — you can switch to PyTorch if preferred)\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout,Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b71c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 6. NEURAL NETWORKS / DEEP LEARNING ======================\n",
    "# PyTorch Forecasting (TFT + others)\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss \n",
    "\n",
    "# PyTorch Lightning (for custom models if needed)\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# GluonTS (alternative DeepAR implementation)\n",
    "from gluonts.torch.model.deepar import DeepAREstimator   \n",
    "\n",
    "# ====================== 7. PATTERN DETECTION (Bonus – highly recommended) ======================\n",
    "import stumpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783b493",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Pipelining (Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 1. LOAD DATA ======================\n",
    "df = pd.read_csv('major-tech-stock-2019-2024.csv')\n",
    "print(f\"Raw data shape: {df.shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "print(df.head(), \"\\n\")\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 2. INITIAL CLEANING ======================\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = [col.strip().replace(' ', '_').replace('Adj_Close', 'Adj Close') for col in df.columns]\n",
    "\n",
    "# Set index and sort\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Drop completely duplicate rows\n",
    "initial_rows = len(df)\n",
    "df.drop_duplicates(subset=['Date', 'Ticker'], keep='first', inplace=True)\n",
    "print(f\"Removed {initial_rows - len(df):,} full duplicate rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing Date or Ticker\n",
    "df.dropna(subset=['Date', 'Ticker'], inplace=True)\n",
    "\n",
    "# Fill or drop missing values intelligently\n",
    "print(f\"\\nMissing values before cleaning:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0998c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill typical missing OHLCV (happens on holidays/weekends per ticker)\n",
    "df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']] = \\\n",
    "    df.groupby('Ticker')[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].ffill()\n",
    "\n",
    "# If still missing (e.g. first rows), backfill\n",
    "df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']] = \\\n",
    "    df.groupby('Ticker')[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].bfill()\n",
    "\n",
    "# Volume = 0 or NaN → set to 0 (some datasets mark holidays as 0)\n",
    "df['Volume'] = df['Volume'].fillna(0)\n",
    "\n",
    "print(f\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 3. ENHANCED FEATURE ENGINEERING ======================\n",
    "\n",
    "df['Year']        = df['Date'].dt.year\n",
    "df['Month']       = df['Date'].dt.month\n",
    "df['Day']         = df['Date'].dt.day\n",
    "df['DayOfWeek']   = df['Date'].dt.dayofweek          # Monday=0, Sunday=6\n",
    "df['Is_Month_End'] = df['Date'].dt.is_month_end.astype(int)\n",
    "df['Is_Month_Start'] = df['Date'].dt.is_month_start.astype(int)\n",
    "\n",
    "# Daily return from Open to Close (intraday momentum)\n",
    "df['Daily_Return'] = (df['Close'] - df['Open']) / df['Open']\n",
    "\n",
    "# Moving averages\n",
    "df['MA7']  = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['MA30'] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(window=30).mean())\n",
    "df['MA90'] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(window=90).mean())\n",
    "# 30-day realized volatility on intraday returns\n",
    "df['Volatility_30d'] = df.groupby('Ticker')['Daily_Return'].transform(\n",
    "    lambda x: x.rolling(window=30, min_periods=10).std()\n",
    ")\n",
    "\n",
    "# Lagged prices (very powerful for ML models)\n",
    "df['Lag_Close_1'] = df.groupby('Ticker')['Close'].shift(1)\n",
    "df['Lag_Close_2'] = df.groupby('Ticker')['Close'].shift(2)\n",
    "df['Lag_Close_3'] = df.groupby('Ticker')['Close'].shift(3)\n",
    "df['Lag_Close_5'] = df.groupby('Ticker')['Close'].shift(5)\n",
    "\n",
    "# Price momentum\n",
    "df['Momentum_5d']  = df.groupby('Ticker')['Close'].transform(lambda x: x.pct_change(5))\n",
    "df['Momentum_10d'] = df.groupby('Ticker')['Close'].transform(lambda x: x.pct_change(10))\n",
    "\n",
    "# Price position within recent range\n",
    "df['Price_Position_20d'] = (\n",
    "    df['Close'] - df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(20).min())\n",
    ") / (\n",
    "    df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(20).max()) - \n",
    "    df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(20).min()) + 1e-8\n",
    ")\n",
    "\n",
    "# Volume features\n",
    "df['Volume_MA20'] = df.groupby('Ticker')['Volume'].transform(lambda x: x.rolling(20).mean())\n",
    "df['Volume_Ratio'] = df['Volume'] / (df['Volume_MA20'] + 1e-6)\n",
    "\n",
    "# High-Low range & typical price\n",
    "df['HL_Range'] = df['High'] - df['Low']\n",
    "df['Typical_Price'] = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "\n",
    "# Overnight gap (useful for next-day prediction)\n",
    "df['Overnight_Gap'] = df.groupby('Ticker')['Open'].pct_change()\n",
    "\n",
    "# Target variable (what most models will predict)\n",
    "# Option 1: Next-day close price direction (classification)\n",
    "df['Target_Direction'] = (df.groupby('Ticker')['Close'].shift(-1) > df['Close']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f82499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== FINAL CLEANING AFTER FEATURES ======================\n",
    "print(f\"\\nRows before dropping NaNs: {len(df)}\")\n",
    "df_clean = df.dropna().reset_index(drop=True)\n",
    "print(f\"Rows after dropping NaNs: {len(df_clean)}\")\n",
    "print(f\"Final shape: {df_clean.shape}\")\n",
    "print(f\"Date range: {df_clean['Date'].min().date()} to {df_clean['Date'].max().date()}\")\n",
    "df_clean.to_csv('tech_stocks_final_clean_engineered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7fa4b",
   "metadata": {},
   "source": [
    "# 3. Cross-Stock COMPARISON ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean.copy()\n",
    "print(df.head())\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(f\"Tickers in dataset        : {sorted(df['Ticker'].unique())}\")\n",
    "print(f\"Total trading days per stock:\")\n",
    "print(df.groupby('Ticker')['Date'].nunique().sort_values(ascending=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────\n",
    "# 1. TOTAL RETURN OVER THE PERIOD\n",
    "# ──────────────────────────────────────\n",
    "print(\"1. CUMULATIVE TOTAL RETURN (Buy & Hold)\")\n",
    "total_returns = df.groupby('Ticker').apply(\n",
    "    lambda x: (x['Adj Close'].iloc[-1] / x['Adj Close'].iloc[0] - 1) * 100\n",
    ").round(2).sort_values(ascending=False)\n",
    "\n",
    "print(total_returns.to_string())\n",
    "print(f\"→ Best performer : {total_returns.idxmax()} (+{total_returns.max():.1f}%)\")\n",
    "print(f\"→ Worst performer: {total_returns.idxmin()} ({total_returns.min():+.1f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────\n",
    "# 2. ANNUALIZED RETURN & VOLATILITY (Sharpe proxy)\n",
    "# ──────────────────────────────────────\n",
    "print(\"2. ANNUALIZED RETURN & RISK\")\n",
    "stats = df.groupby('Ticker').agg(\n",
    "    Annualized_Return_pct = ('Daily_Return', lambda x: (1 + x.mean())**252 - 1),\n",
    "    Annualized_Volatility = ('Daily_Return', lambda x: x.std() * np.sqrt(252)),\n",
    "    Sharpe_Ratio          = ('Daily_Return', lambda x: (x.mean() / x.std()) * np.sqrt(252)),\n",
    "    Max_Drawdown_pct      = ('Adj Close', lambda x: ((x.cummax() - x)/x.cummax()).max())\n",
    ").round(4) * 100\n",
    "\n",
    "stats['Annualized_Return_pct'] = stats['Annualized_Return_pct'].round(2)\n",
    "stats['Max_Drawdown_pct'] = stats['Max_Drawdown_pct'].round(2)\n",
    "stats = stats.sort_values('Sharpe_Ratio', ascending=False)\n",
    "\n",
    "print(stats)\n",
    "print(f\"→ Highest Sharpe       : {stats.index[0]} ({stats['Sharpe_Ratio'].iloc[0]:.3f})\")\n",
    "print(f\"→ Lowest risk-adjusted: {stats.index[-1]} ({stats['Sharpe_Ratio'].iloc[-1]:.3f})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────\n",
    "# 3. CORRELATION MATRIX (Daily Returns)\n",
    "# ──────────────────────────────────────\n",
    "print(\"3. DAILY RETURN CORRELATION MATRIX\")\n",
    "corr_matrix = df.pivot(index='Date', columns='Ticker', values='Daily_Return').corr()\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "print(f\"→ Highest correlation : {corr_matrix.stack().idxmax()} = {corr_matrix.stack().max():.3f}\")\n",
    "print(f\"→ Lowest correlation  : {corr_matrix.stack().idxmin()} = {corr_matrix.stack().min():.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c453ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='Blues', annot_kws={\"color\": \"Black\"})\n",
    "plt.title('Ticker Correlation Matrix')\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "plt.title('Ticker Correlation Matrix', color='white')\n",
    "plt.xticks(color='white')\n",
    "plt.yticks(color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a20386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────\n",
    "# 4. TRADING ACTIVITY COMPARISON\n",
    "# ──────────────────────────────────────\n",
    "print(\"4. AVERAGE DAILY VOLUME & VOLATILITY RANK\")\n",
    "volume_vol = df.groupby('Ticker').agg(\n",
    "    Avg_Daily_Volume_M = ('Volume', 'mean'),\n",
    "    Volatility_30d_Avg = ('Volatility_30d', 'mean'),\n",
    "    Pct_Days_Above_5pct_Move = ('Daily_Return', lambda x: (x.abs() > 0.05).mean())\n",
    ").round(6)\n",
    "\n",
    "volume_vol['Avg_Daily_Volume_M'] /= 1e6\n",
    "volume_vol['Pct_Days_Above_5pct_Move'] *= 100\n",
    "volume_vol = volume_vol.round(3).sort_values('Volatility_30d_Avg', ascending=False)\n",
    "\n",
    "print(volume_vol)\n",
    "print(f\"→ Most volatile stock : {volume_vol.index[0]}\")\n",
    "print(f\"→ Highest trading volume: {volume_vol['Avg_Daily_Volume_M'].idxmax()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780776b",
   "metadata": {},
   "source": [
    "# 4. Self-Comparison Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_stats = df.groupby(['Ticker', 'Year']).agg({\n",
    "    'Daily_Return': ['mean', 'std'],\n",
    "    'Close': ['mean']\n",
    "}).reset_index()\n",
    "print(\"\\n=== YEARLY STATS PER TICKER (SELF OVER TIME) ===\")\n",
    "print(yearly_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ceb89",
   "metadata": {},
   "source": [
    "# 5. CLASSICAL TIME SERIES MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785014f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df['Ticker'].unique()\n",
    "results_summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37190d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────── FUNCTION 1: ADF TEST ──────────────────────\n",
    "def adf_test(series, name=\"Series\"):\n",
    "    result = adfuller(series, regression='c')\n",
    "    print(f\"  ADF Test ({name})\")\n",
    "    print(f\"    ADF Statistic : {result[0]:.6f}\")\n",
    "    print(f\"    p-value       : {result[1]:.6f}\")\n",
    "    print(f\"    Critical 1%   : {result[4]['1%']:.3f} | 5%: {result[4]['5%']:.3f} | 10%: {result[4]['10%']:.3f}\")\n",
    "    return result[1] < 0.05, result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────── FUNCTION 2: SARIMA (auto_arima) ──────────────────────\n",
    "def fit_sarima(series):\n",
    "    try:\n",
    "        model = auto_arima(\n",
    "            series,\n",
    "            start_p=0, max_p=3,\n",
    "            start_q=0, max_q=3,\n",
    "            start_P=0, max_P=2,\n",
    "            start_Q=0, max_Q=2,\n",
    "            m=5, seasonal=True,\n",
    "            stepwise=True,\n",
    "            suppress_warnings=True,\n",
    "            error_action='ignore',\n",
    "            trace=False\n",
    "        )\n",
    "        print(f\"    Best SARIMA: {model.order} x {model.seasonal_order} (AIC: {model.aic():.2f})\")\n",
    "        return model.aic(), model.order, model.seasonal_order\n",
    "    except:\n",
    "        print(\"    auto_arima failed → skipped\")\n",
    "        return np.inf, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faebd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────── FUNCTION 3: PROPHET ──────────────────────\n",
    "def fit_prophet(df_prophet):\n",
    "    m = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        seasonality_mode='multiplicative'\n",
    "    )\n",
    "    m.add_country_holidays(country_name='US')\n",
    "    m.fit(df_prophet)\n",
    "    \n",
    "    future = m.make_future_dataframe(periods=60)\n",
    "    forecast = m.predict(future)\n",
    "    \n",
    "    # In-sample MAPE\n",
    "    y_true = df_prophet['y'].values\n",
    "    y_pred = forecast.iloc[:-60]['yhat'].values\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    print(f\"    Prophet In-sample MAPE: {mape:.2f}%\")\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98016e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────── MAIN LOOP ──────────────────────\n",
    "results = []\n",
    "for ticker in df_clean['Ticker'].unique():\n",
    "    print(f\"\\n{'─'*25} TICKER: {ticker.upper()} {'─'*25}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    ts = df_clean[df_clean['Ticker'] == ticker].copy()\n",
    "    ts = ts.sort_values('Date').set_index('Date')\n",
    "    \n",
    "    price = ts['Adj Close']\n",
    "    # Use proper log returns (Close-to-Close)\n",
    "    log_returns = np.log(ts['Adj Close'] / ts['Adj Close'].shift(1)).dropna()\n",
    "\n",
    "    # 1. ADF Tests\n",
    "    stationary_price, p_price = adf_test(price, \"Adj Close Price\")\n",
    "    stationary_ret, p_ret = adf_test(log_returns, \"Log Returns\")\n",
    "\n",
    "    # Choose series for SARIMA\n",
    "    if stationary_price:\n",
    "        model_series = price\n",
    "        series_name = \"Adj Close (stationary)\"\n",
    "    else:\n",
    "        model_series = log_returns\n",
    "        series_name = \"Log Returns (stationary)\"\n",
    "\n",
    "    # 2. SARIMA\n",
    "    sarima_aic, sarima_order, sarima_seasonal = fit_sarima(model_series)\n",
    "\n",
    "    # 3. Prophet (on price level)\n",
    "    prophet_df = ts.reset_index()[['Date', 'Adj Close']].rename(columns={'Date': 'ds', 'Adj Close': 'y'})\n",
    "    prophet_mape = fit_prophet(prophet_df)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Ticker'           : ticker,\n",
    "        'Price_Stationary' : 'Yes' if stationary_price else 'No',\n",
    "        'Price_p_value'    : round(p_price, 6),\n",
    "        'Series_Used'      : series_name,\n",
    "        'SARIMA_AIC'       : round(sarima_aic, 2) if sarima_aic != np.inf else 'Failed',\n",
    "        'Prophet_MAPE_%'   : round(prophet_mape, 2)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10516d7d",
   "metadata": {},
   "source": [
    "# 6. SUPERVISED MACHINE LEARNING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = df_clean.copy()\n",
    "df_ml['Target_Next_Price'] = df_ml.groupby('Ticker')['Adj Close'].shift(-1)\n",
    "df_ml['Target_Direction'] = (df_ml['Target_Next_Price'] > df_ml['Adj Close']).astype(int)\n",
    "df_ml = df_ml.dropna(subset=['Target_Next_Price']).reset_index(drop=True)\n",
    "\n",
    "feature_cols = [\n",
    "    'Open', 'High', 'Low', 'Volume', 'Year', 'Month', 'Day', 'DayOfWeek',\n",
    "    'Is_Month_End', 'Is_Month_Start', 'Daily_Return', 'MA7', 'MA30', 'MA90',\n",
    "    'Volatility_30d', 'Lag_Close_1', 'Lag_Close_2', 'Lag_Close_3', 'Lag_Close_5',\n",
    "    'Momentum_5d', 'Momentum_10d', 'Price_Position_20d',\n",
    "    'Volume_MA20', 'Volume_Ratio', 'HL_Range', 'Typical_Price', 'Overnight_Gap'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634166a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = df_ml[feature_cols]\n",
    "y = df_ml['Target_Next_Price']\n",
    "dates = df_ml['Date']\n",
    "tickers = df_ml['Ticker']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_raw)\n",
    "X = pd.DataFrame(X, columns=feature_cols)\n",
    "\n",
    "# Train-test split by time (last 20% as test)\n",
    "split_idx = int(len(df_ml) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "dates_test = dates.iloc[split_idx:]\n",
    "tickers_test = tickers.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training samples  : {len(X_train):,}\")\n",
    "print(f\"Test samples      : {len(X_test):,}\")\n",
    "print(f\"Test date range   : {dates_test.min().date()} → {dates_test.max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca93e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1: Linear Regression\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "pred_lr = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Metrics\n",
    "mae = mean_absolute_error(y_test, pred_lr)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred_lr))\n",
    "mape = mean_absolute_percentage_error(y_test, pred_lr) * 100\n",
    "r2 = r2_score(y_test, pred_lr)\n",
    "evs = explained_variance_score(y_test, pred_lr)\n",
    "max_err = max_error(y_test, pred_lr)\n",
    "\n",
    "print(f\"MAE                    : {mae:.3f}\")\n",
    "print(f\"RMSE                   : {rmse:.3f}\")\n",
    "print(f\"MAPE                   : {mape:.2f}%\")\n",
    "print(f\"R² Score               : {r2:.4f}\")\n",
    "print(f\"Explained Variance     : {evs:.4f}\")\n",
    "print(f\"Max Error              : {max_err:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions\n",
    "print(\"\\nSample Predictions (Last 10 days):\")\n",
    "results_lr = pd.DataFrame({\n",
    "    'Date': dates_test.reset_index(drop=True),\n",
    "    'Ticker': tickers_test.reset_index(drop=True),\n",
    "    'Actual': y_test.reset_index(drop=True),\n",
    "    'Predicted': pred_lr\n",
    "})\n",
    "results_lr['Error'] = results_lr['Actual'] - results_lr['Predicted']\n",
    "print(results_lr.tail(10).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae096426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(results_lr['Date'], results_lr['Actual'], label='Actual Price', alpha=0.8)\n",
    "plt.plot(results_lr['Date'], results_lr['Predicted'], label='Linear Reg Predicted', alpha=0.8)\n",
    "plt.title('Linear Regression: Actual vs Predicted Next-Day Price', fontsize=14)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Adjusted Close Price')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc758756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2: Ridge Regression\n",
    "model_ridge = Ridge(alpha=1.0)\n",
    "model_ridge.fit(X_train, y_train)\n",
    "pred_ridge = model_ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, pred_ridge)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred_ridge))\n",
    "mape = mean_absolute_percentage_error(y_test, pred_ridge) * 100\n",
    "r2 = r2_score(y_test, pred_ridge)\n",
    "evs = explained_variance_score(y_test, pred_ridge)\n",
    "max_err = max_error(y_test, pred_ridge)\n",
    "\n",
    "print(f\"MAE                    : {mae:.3f}\")\n",
    "print(f\"RMSE                   : {rmse:.3f}\")\n",
    "print(f\"MAPE                   : {mape:.2f}%\")\n",
    "print(f\"R² Score               : {r2:.4f}\")\n",
    "print(f\"Explained Variance     : {evs:.4f}\")\n",
    "print(f\"Max Error              : {max_err:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0319cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ridge = pd.DataFrame({\n",
    "    'Date': dates_test.reset_index(drop=True),\n",
    "    'Actual': y_test.reset_index(drop=True),\n",
    "    'Predicted': pred_ridge\n",
    "})\n",
    "results_ridge['Error'] = results_ridge['Actual'] - results_ridge['Predicted']\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(results_ridge.tail(10).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8801e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(results_ridge['Date'], results_ridge['Actual'], label='Actual', alpha=0.8)\n",
    "plt.plot(results_ridge['Date'], results_ridge['Predicted'], label='Ridge Predicted', color='green', alpha=0.8)\n",
    "plt.title('Ridge Regression: Actual vs Predicted', fontsize=14)\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d0c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 3: Lasso Regression\n",
    "model_lasso = Lasso(alpha=0.001, max_iter=20000)\n",
    "model_lasso.fit(X_train, y_train)\n",
    "pred_lasso = model_lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45265f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, pred_lasso)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred_lasso))\n",
    "mape = mean_absolute_percentage_error(y_test, pred_lasso) * 100\n",
    "r2 = r2_score(y_test, pred_lasso)\n",
    "evs = explained_variance_score(y_test, pred_lasso)\n",
    "max_err = max_error(y_test, pred_lasso)\n",
    "\n",
    "print(f\"MAE                    : {mae:.3f}\")\n",
    "print(f\"RMSE                   : {rmse:.3f}\")\n",
    "print(f\"MAPE                   : {mape:.2f}%\")\n",
    "print(f\"R² Score               : {r2:.4f}\")\n",
    "print(f\"Explained Variance     : {evs:.4f}\")\n",
    "print(f\"Max Error              : {max_err:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2875f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lasso = pd.DataFrame({\n",
    "    'Date': dates_test.reset_index(drop=True), \n",
    "    'Actual': y_test.reset_index(drop=True), \n",
    "    'Predicted': pred_lasso\n",
    "})\n",
    "results_lasso['Error'] = results_lasso['Actual'] - results_lasso['Predicted']\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(results_lasso.tail(10).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(results_lasso['Date'], results_lasso['Actual'], label='Actual')\n",
    "plt.plot(results_lasso['Date'], results_lasso['Predicted'], label='Lasso Predicted', color='red')\n",
    "plt.title('Lasso Regression: Actual vs Predicted'); \n",
    "plt.legend(); \n",
    "plt.grid(); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1853fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 4: Random Forest Regressor\n",
    "model_rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "model_rf.fit(X_train, y_train)\n",
    "pred_rf = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9cea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, pred_rf)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred_rf))\n",
    "mape = mean_absolute_percentage_error(y_test, pred_rf) * 100\n",
    "r2 = r2_score(y_test, pred_rf)\n",
    "evs = explained_variance_score(y_test, pred_rf)\n",
    "max_err = max_error(y_test, pred_rf)\n",
    "\n",
    "print(f\"MAE                    : {mae:.3f}\")\n",
    "print(f\"RMSE                   : {rmse:.3f}\")\n",
    "print(f\"MAPE                   : {mape:.2f}%\")\n",
    "print(f\"R² Score               : {r2:.4f}\")\n",
    "print(f\"Explained Variance     : {evs:.4f}\")\n",
    "print(f\"Max Error              : {max_err:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27515494",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = pd.DataFrame({\n",
    "    'Date': dates_test.reset_index(drop=True), \n",
    "    'Actual': y_test.reset_index(drop=True), \n",
    "    'Predicted': pred_rf\n",
    "})\n",
    "results_rf['Error'] = results_rf['Actual'] - results_rf['Predicted']\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(results_rf.tail(10).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ba9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(results_rf['Date'], results_rf['Actual'], label='Actual', linewidth=1.5)\n",
    "plt.plot(results_rf['Date'], results_rf['Predicted'], label='Random Forest Predicted', color='darkorange', alpha=0.9)\n",
    "plt.title('Random Forest: Best Performing Model', fontsize=16, fontweight='bold')\n",
    "plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27140d",
   "metadata": {},
   "source": [
    "# 7. Gradient Booster MACHINE LEARNING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1abb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LIGHTGBM REGRESSOR\n",
    "# LightGBM Dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data  = lgb.Dataset(X_test,  label=y_test, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',    # We want to predict continuous values\n",
    "    'metric': 'mae',              # Mean Absolute Error — both optimization and evaluation metric\n",
    "    'boosting_type': 'gbdt',      # Classic gradient boosting decision trees\n",
    "    'num_leaves': 31,             # Max leaves per tree (31 is default, good starting point)\n",
    "    'learning_rate': 0.05,        # Small step size → slower but more accurate learning\n",
    "    'feature_fraction': 0.9,      # Randomly select 90% of features at each tree (reduces overfitting)\n",
    "    'bagging_fraction': 0.8,      # Randomly select 80% of data for each tree (like subsample)\n",
    "    'bagging_freq': 5,            # Perform bagging every 5 iterations\n",
    "    'verbose': -1,                # Suppress most warnings/info\n",
    "    'seed': 42                    # For reproducibility\n",
    "}\n",
    "\n",
    "# === TRAIN WITH CALLBACK (ONLY WORKING METHOD IN 2025+) ===\n",
    "model_lgb = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=2000,                 # Maximum number of trees (iterations)\n",
    "    valid_sets=[test_data],               # Datasets to evaluate during training\n",
    "    valid_names=['valid'],                # Name shown in logs\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# === PREDICT ===\n",
    "best_iter = model_lgb.best_iteration\n",
    "pred_lgb = model_lgb.predict(X_test, num_iteration=best_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefdde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae  = mean_absolute_error(y_test, pred_lgb)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred_lgb))\n",
    "mape = mean_absolute_percentage_error(y_test, pred_lgb) * 100\n",
    "r2   = r2_score(y_test, pred_lgb)\n",
    "evs  = explained_variance_score(y_test, pred_lgb)\n",
    "maxe = max_error(y_test, pred_lgb)\n",
    "\n",
    "print(f\"\\nLightGBM FINAL RESULTS (Best Iteration: {best_iter}):\")\n",
    "print(f\"   MAE                : {mae:.3f}\")\n",
    "print(f\"   RMSE               : {rmse:.3f}\")\n",
    "print(f\"   MAPE               : {mape:.2f}%\")\n",
    "print(f\"   R² Score           : {r2:.4f}\")\n",
    "print(f\"   Explained Variance : {evs:.4f}\")\n",
    "print(f\"   Max Error          : {maxe:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OUTPUT TABLE ===\n",
    "results = pd.DataFrame({\n",
    "    'Date': dates_test,\n",
    "    'Ticker': tickers_test,\n",
    "    'Actual': y_test.round(2).values,\n",
    "    'Predicted': np.round(pred_lgb, 2),\n",
    "    'Error': np.round(y_test.values - pred_lgb, 2)\n",
    "})\n",
    "print(\"\\nLast 10 Predictions:\")\n",
    "print(results.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1154973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLOT ===\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(results['Date'], results['Actual'], label='Actual Price', linewidth=2, color='steelblue')\n",
    "plt.plot(results['Date'], results['Predicted'], label='LightGBM Predicted', linewidth=2, color='darkgreen', alpha=0.9)\n",
    "plt.title('LightGBM – Best Model So Far (All 5 Tech Stocks)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Adjusted Close Price ($)')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf12c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 6: XGBoost Regressor\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)   # Training data\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test)    # Test/validation data (used for early stopping)\n",
    "\n",
    "params = {\n",
    "    'objective'        : 'reg:squarederror',   # We want to minimize squared error\n",
    "    'eval_metric'      : 'mae',                # Monitor Mean Absolute Error during training\n",
    "    'eta'              : 0.05,                 # Learning rate (smaller = more robust)\n",
    "    'max_depth'        : 6,                    # Max tree depth (6–8 is good for tabular data)\n",
    "    'subsample'        : 0.8,                  # Use 80% of rows per tree → reduces overfitting\n",
    "    'colsample_bytree' : 0.8,                  # Use 80% of features per tree\n",
    "    'seed'             : 42,                   # For reproducibility\n",
    "    'verbosity'        : 0                     # 0 = silent, 1 = info, 2 = warning\n",
    "}\n",
    "\n",
    "# We allow up to 2000 trees, but stop if no improvement in 100 rounds\n",
    "model_xgb = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=2000,                     # Maximum number of trees\n",
    "    evals=[(dtest, 'valid')],                 # Validation set (named 'valid')\n",
    "    early_stopping_rounds=100,                # Stop if MAE doesn't improve for 100 rounds\n",
    "    verbose_eval=100                          # Print progress every 100 rounds\n",
    ")\n",
    "\n",
    "# Best iteration is automatically saved\n",
    "best_iter = model_xgb.best_iteration\n",
    "print(f\"\\nXGBoost stopped early at iteration {best_iter} (saved ~{2000 - best_iter} trees)\")\n",
    "pred_xgb = model_xgb.predict(dtest, iteration_range=(0, best_iter + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb943a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae  = mean_absolute_error(y_test, pred_xgb)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred_xgb))\n",
    "mape = mean_absolute_percentage_error(y_test, pred_xgb) * 100\n",
    "r2   = r2_score(y_test, pred_xgb)\n",
    "evs  = explained_variance_score(y_test, pred_xgb)\n",
    "maxe = max_error(y_test, pred_xgb)\n",
    "\n",
    "print(f\"\\nXGBOOST FINAL RESULTS (Best Iteration: {best_iter}):\")\n",
    "print(f\"   MAE                : {mae:.3f}\")\n",
    "print(f\"   RMSE               : {rmse:.3f}\")\n",
    "print(f\"   MAPE               : {mape:.2f}%\")\n",
    "print(f\"   R² Score           : {r2:.4f}\")\n",
    "print(f\"   Explained Variance : {evs:.4f}\")\n",
    "print(f\"   Max Error          : {maxe:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e191b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_xgb = pd.DataFrame({\n",
    "    'Date'      : dates_test.reset_index(drop=True),\n",
    "    'Ticker'    : tickers_test.reset_index(drop=True),\n",
    "    'Actual'    : np.round(y_test.values, 2),\n",
    "    'Predicted' : np.round(pred_xgb, 2),\n",
    "    'Error'     : np.round(y_test.values - pred_xgb, 2)\n",
    "})\n",
    "\n",
    "print(\"\\nLast 10 Predictions:\")\n",
    "print(results_xgb.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3.5 ))\n",
    "plt.plot(results_xgb['Date'], results_xgb['Actual'], label='Actual Price', linewidth=2, color='steelblue')\n",
    "plt.plot(results_xgb['Date'], results_xgb['Predicted'], \n",
    "         label='XGBoost Predicted', linewidth=2, color='crimson', alpha=0.9)\n",
    "plt.title('XGBoost Regressor – Next-Day Price Prediction (All 5 Tech Stocks)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Adjusted Close Price ($)')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d52b8",
   "metadata": {},
   "source": [
    "# 8. UNSUPERVISED MACHINE LEARNING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d95491",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_unsup = [\n",
    "    'Open', 'High', 'Low', 'Volume', 'Daily_Return', 'Volatility_30d',\n",
    "    'MA7', 'MA30', 'MA90', 'Lag_Close_1', 'Lag_Close_2', 'Lag_Close_3', 'Lag_Close_5',\n",
    "    'Momentum_5d', 'Momentum_10d', 'Price_Position_20d',\n",
    "    'Volume_MA20', 'Volume_Ratio', 'HL_Range', 'Typical_Price', 'Overnight_Gap',\n",
    "    'Year', 'Month', 'DayOfWeek'\n",
    "]\n",
    "\n",
    "X_unsup_raw = df_clean[feature_cols_unsup]\n",
    "X_unsup = pd.DataFrame(StandardScaler().fit_transform(X_unsup_raw), columns=feature_cols_unsup)\n",
    "print(f\"Unsupervised dataset: {X_unsup.shape[0]:,} samples × {X_unsup.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE 1: CLUSTERING – K-Means (Market Regime Detection)\n",
    "# Fit K-Means with 4 clusters (typical for market regimes: bull/bear/sideways/high-vol)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "df_clean['KMeans_Regime'] = kmeans.fit_predict(X_unsup)\n",
    "\n",
    "# Metrics\n",
    "sil_score = silhouette_score(X_unsup, df_clean['KMeans_Regime'])\n",
    "print(f\"Silhouette Score: {sil_score:.4f} (0.5+ = good separation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked bar chart: Time spent in each regime per stock\n",
    "regime_counts = df_clean.groupby(['Ticker', 'KMeans_Regime']).size().unstack(fill_value=0)\n",
    "regime_counts = regime_counts[sorted(regime_counts.columns)]\n",
    "\n",
    "plt.figure(figsize=(6, 3.5))\n",
    "regime_counts.plot(kind='barh', stacked=True, cmap='tab10', ax=plt.gca(), linewidth=1, edgecolor='white')\n",
    "plt.title('Market Regimes by Stock – Time Spent in Each Regime', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Trading Days')\n",
    "plt.ylabel('Ticker')\n",
    "plt.legend(title='Regime', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRegime Duration Table:\")\n",
    "print(regime_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE 2: ANOMALY DETECTION – Isolation Forest (Flash Crashes & Spikes)\n",
    "\n",
    "# Detect anomalies (2% contamination = ~1 extreme day per month)\n",
    "iso_forest = IsolationForest(contamination=0.02, random_state=42, n_jobs=-1)\n",
    "df_clean['Is_Anomaly'] = iso_forest.fit_predict(X_unsup)\n",
    "\n",
    "# Metrics & counts\n",
    "anomaly_rate = (df_clean['Is_Anomaly'] == -1).mean() * 100\n",
    "print(f\"Anomaly Detection Rate: {anomaly_rate:.2f}% ({len(df_clean[df_clean['Is_Anomaly'] == -1]):,} extreme days detected)\")\n",
    "print(f\"False positive rate: {anomaly_rate:.2f}% (adjust contamination to change)\")\n",
    "\n",
    "# Show top anomalies\n",
    "anomalies = df_clean[df_clean['Is_Anomaly'] == -1].copy()\n",
    "anomalies = anomalies.sort_values('Daily_Return', key=abs, ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 Extreme Days Detected:\")\n",
    "print(anomalies[['Date', 'Ticker', 'Daily_Return', 'Volume_Ratio']].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calendar heatmap of anomalies\n",
    "anomaly_heatmap = df_clean[df_clean['Is_Anomaly'] == -1].copy()\n",
    "anomaly_heatmap['YearMonth'] = anomaly_heatmap['Date'].dt.to_period('M').astype(str)\n",
    "heatmap_data = anomaly_heatmap.groupby(['YearMonth', 'Ticker']).size().unstack(fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, cmap=\"Reds\", annot=True, fmt=\"d\", linewidths=0.5, cbar_kws={'label': 'Anomalies'})\n",
    "plt.title('Anomaly Calendar – Extreme Market Days by Month & Stock', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Ticker')\n",
    "plt.ylabel('Year-Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26258b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE 3: DIMENSIONALITY REDUCTION – PCA (Feature Compression)\n",
    "# Fit PCA (keep all components for analysis)\n",
    "pca_model = PCA(random_state=42)\n",
    "pca_components = pca_model.fit_transform(X_unsup)\n",
    "explained_var = pca_model.explained_variance_ratio_\n",
    "cum_var = np.cumsum(explained_var)\n",
    "\n",
    "# Components needed for 95% variance\n",
    "n_95 = np.argmax(cum_var >= 0.95) + 1\n",
    "print(f\"Components for 95% variance: {n_95}/{len(explained_var)}\")\n",
    "print(f\"First 3 components explain: {cum_var[2]*100:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot with cumulative line\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.bar(range(1, 11), explained_var[:10], alpha=0.7, color='steelblue', label='Individual Variance')\n",
    "plt.plot(range(1, 11), cum_var[:10], 'ro-', linewidth=2, markersize=6, label='Cumulative')\n",
    "plt.axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95% Threshold')\n",
    "plt.title('PCA Scree Plot – Market Feature Compression', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features driving PC1 (market regime indicator)\n",
    "pc1_loadings = pd.Series(pca_model.components_[0], index=feature_cols_unsup).abs().nlargest(8)\n",
    "plt.figure(figsize=(5, 5))\n",
    "pc1_loadings.plot(kind='bar', color='coral', alpha=0.8)\n",
    "plt.title('Top 8 Features Driving PC1 (Market Regime Factor)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Absolute Loading')\n",
    "plt.xlabel('Feature')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE 4: DENSITY ESTIMATION – Gaussian Mixture Model (Market States)\n",
    "\n",
    "# Fit GMM (4 components = soft clustering)\n",
    "gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n",
    "df_clean['GMM_State'] = gmm.fit_predict(X_unsup)\n",
    "probs = pd.DataFrame(gmm.predict_proba(X_unsup),\n",
    "                     columns=[f'State_{i}' for i in range(4)])\n",
    "prob_df = pd.concat([df_clean[['Date','Ticker']].reset_index(drop=True), probs], axis=1)\n",
    "\n",
    "gmm_sil = silhouette_score(X_unsup, df_clean['GMM_State'])\n",
    "print(f\"GMM Silhouette Score: {gmm_sil:.4f}\")\n",
    "print(f\"Log Likelihood: {gmm.lower_bound_:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily average probabilities per state (only numeric columns)\n",
    "daily_avg = prob_df.groupby('Date').mean(numeric_only=True)  # Key fix\n",
    "overall_mean = daily_avg.mean()  # Now safe: all columns are numeric\n",
    "\n",
    "# For nicer state names (optional)\n",
    "state_names = ['State 0', 'State 1', 'State 2', 'State 3']\n",
    "\n",
    "# Your 4 original colors\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Vertical bar chart\n",
    "plt.figure(figsize=(5, 5))\n",
    "bars = plt.bar(state_names, overall_mean.values, \n",
    "               color=colors, edgecolor='black', linewidth=1.2, alpha=0.85)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.title('Average Daily Probability of Each Market State (2019–2023)\\nGaussian Mixture Model (4 Components)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Market State', fontsize=12)\n",
    "plt.ylabel('Mean Probability', fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11466823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show state transitions per ticker\n",
    "print(\"\\nMarket State Distribution per Stock:\")\n",
    "state_dist = df_clean.groupby('Ticker')['GMM_State'].value_counts(normalize=True).unstack(fill_value=0).round(3)\n",
    "print(state_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b67f99",
   "metadata": {},
   "source": [
    "# 9. Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4788199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1: MLP Regressor (scikit-learn) – Fast & Clean\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50, 25),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=20\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "pred_mlp = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, pred_mlp)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred_mlp))\n",
    "mape = mean_absolute_percentage_error(y_test, pred_mlp) * 100\n",
    "r2 = r2_score(y_test, pred_mlp)\n",
    "\n",
    "print(f\"MLP Results → MAE: {mae:.3f} | RMSE: {rmse:.3f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n",
    "print(f\"Converged in {mlp.n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cddd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "results_mlp = pd.DataFrame({'Date': dates_test, 'Actual': y_test, 'Predicted': pred_mlp})\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(results_mlp['Date'], results_mlp['Actual'], label='Actual Price', linewidth=2)\n",
    "plt.plot(results_mlp['Date'], results_mlp['Predicted'], label='MLP Predicted', color='red', alpha=0.8)\n",
    "plt.title('MLP Regressor (scikit-learn) – Next-Day Price Prediction', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend(); \n",
    "plt.grid(alpha=0.3); \n",
    "plt.tight_layout(); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5068b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2: Simple Keras Neural Network (2 Hidden Layers)\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output: next-day price\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict\n",
    "pred_keras = model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, pred_keras)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred_keras))\n",
    "mape = mean_absolute_percentage_error(y_test, pred_keras) * 100\n",
    "r2 = r2_score(y_test, pred_keras)\n",
    "\n",
    "print(f\"\\nKeras NN → MAE: {mae:.3f} | RMSE: {rmse:.3f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n",
    "print(f\"Best epoch: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training + prediction\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax1.plot(history.history['loss'], label='Training Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('Keras Training History'); ax1.legend(); ax1.grid(alpha=0.3)\n",
    "\n",
    "results_keras = pd.DataFrame({'Date': dates_test, 'Actual': y_test, 'Predicted': pred_keras})\n",
    "ax2.plot(results_keras['Date'], results_keras['Actual'], label='Actual')\n",
    "ax2.plot(results_keras['Date'], results_keras['Predicted'], label='Keras Predicted', color='purple')\n",
    "ax2.set_title('Keras Neural Network – Prediction'); ax2.legend(); ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 3: Wide & Deep Neural Network (Keras Functional API)\n",
    "# Wide path (linear)\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "wide = Dense(64, activation='linear')(input_layer)\n",
    "\n",
    "# Deep path\n",
    "deep = Dense(128, activation='relu')(input_layer)\n",
    "deep = Dropout(0.3)(deep)\n",
    "deep = Dense(64, activation='relu')(deep)\n",
    "deep = Dropout(0.3)(deep)\n",
    "deep = Dense(32, activation='relu')(deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine wide + deep\n",
    "combined = Concatenate()([wide, deep])\n",
    "output = Dense(1)(combined)\n",
    "\n",
    "wide_deep = Model(inputs=input_layer, outputs=output)\n",
    "wide_deep.compile(optimizer=Adam(0.0005), loss='mse', metrics=['mae'])\n",
    "\n",
    "wide_deep.fit(X_train, y_train, validation_split=0.2, epochs=200, batch_size=64,\n",
    "              callbacks=[EarlyStopping(patience=15, restore_best_weights=True)], verbose=1)\n",
    "\n",
    "pred_wd = wide_deep.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ddb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, pred_wd)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred_wd))\n",
    "r2 = r2_score(y_test, pred_wd)\n",
    "\n",
    "print(f\"\\nWide & Deep → MAE: {mae:.3f} | RMSE: {rmse:.3f} | R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(dates_test, y_test, label='Actual', linewidth=2)\n",
    "plt.plot(dates_test, pred_wd, label='Wide & Deep Predicted', color='darkorange', linewidth=2)\n",
    "plt.title('Wide & Deep Neural Network – Best of Both Worlds', fontsize=16, fontweight='bold')\n",
    "plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
